// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: tensorflow/core/protobuf/rewriter_config.proto

package protobuf

import proto "github.com/gogo/protobuf/proto"
import fmt "fmt"
import math "math"
import tensorflow5 "github.com/oracle/graphpipe-go/cmd/graphpipe-tf/internal/github.com/tensorflow/tensorflow/tensorflow/go/core/framework"

import sortkeys "github.com/gogo/protobuf/sortkeys"

import io "io"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

type RewriterConfig_Toggle int32

const (
	RewriterConfig_DEFAULT RewriterConfig_Toggle = 0
	RewriterConfig_ON      RewriterConfig_Toggle = 1
	RewriterConfig_OFF     RewriterConfig_Toggle = 2
	// Enable some aggressive optimizations that use assumptions that TF graphs
	// may break. For example, assume the shape of a placeholder matches its
	// actual feed.
	RewriterConfig_AGGRESSIVE RewriterConfig_Toggle = 3
)

var RewriterConfig_Toggle_name = map[int32]string{
	0: "DEFAULT",
	1: "ON",
	2: "OFF",
	3: "AGGRESSIVE",
}
var RewriterConfig_Toggle_value = map[string]int32{
	"DEFAULT":    0,
	"ON":         1,
	"OFF":        2,
	"AGGRESSIVE": 3,
}

func (x RewriterConfig_Toggle) String() string {
	return proto.EnumName(RewriterConfig_Toggle_name, int32(x))
}
func (RewriterConfig_Toggle) EnumDescriptor() ([]byte, []int) {
	return fileDescriptorRewriterConfig, []int{2, 0}
}

// Enum controlling the number of times to run optimizers. The default is to
// run them once.
type RewriterConfig_NumIterationsType int32

const (
	RewriterConfig_DEFAULT_NUM_ITERS RewriterConfig_NumIterationsType = 0
	RewriterConfig_ONE               RewriterConfig_NumIterationsType = 1
	RewriterConfig_TWO               RewriterConfig_NumIterationsType = 2
)

var RewriterConfig_NumIterationsType_name = map[int32]string{
	0: "DEFAULT_NUM_ITERS",
	1: "ONE",
	2: "TWO",
}
var RewriterConfig_NumIterationsType_value = map[string]int32{
	"DEFAULT_NUM_ITERS": 0,
	"ONE":               1,
	"TWO":               2,
}

func (x RewriterConfig_NumIterationsType) String() string {
	return proto.EnumName(RewriterConfig_NumIterationsType_name, int32(x))
}
func (RewriterConfig_NumIterationsType) EnumDescriptor() ([]byte, []int) {
	return fileDescriptorRewriterConfig, []int{2, 1}
}

type RewriterConfig_MemOptType int32

const (
	// The default setting (SCHEDULING and SWAPPING HEURISTICS only)
	RewriterConfig_DEFAULT_MEM_OPT RewriterConfig_MemOptType = 0
	// Disabled in the meta-optimizer.
	RewriterConfig_NO_MEM_OPT RewriterConfig_MemOptType = 1
	// Driven by manual op-level annotations.
	RewriterConfig_MANUAL RewriterConfig_MemOptType = 2
	// Swapping heuristic will move a tensor from the GPU to the CPU and move
	// it back when needed to reduce peak memory usage.
	RewriterConfig_SWAPPING_HEURISTICS RewriterConfig_MemOptType = 4
	// Recomputation heuristics will recompute ops (such as Relu activation)
	// during backprop instead of storing them, reducing peak memory usage.
	RewriterConfig_RECOMPUTATION_HEURISTICS RewriterConfig_MemOptType = 5
	// Scheduling will split big ops such as AddN and try to enforce a schedule
	// of the new computations that decreases peak memory usage.
	RewriterConfig_SCHEDULING_HEURISTICS RewriterConfig_MemOptType = 6
	// Use any combination of swapping and recomputation heuristics.
	RewriterConfig_HEURISTICS RewriterConfig_MemOptType = 3
)

var RewriterConfig_MemOptType_name = map[int32]string{
	0: "DEFAULT_MEM_OPT",
	1: "NO_MEM_OPT",
	2: "MANUAL",
	4: "SWAPPING_HEURISTICS",
	5: "RECOMPUTATION_HEURISTICS",
	6: "SCHEDULING_HEURISTICS",
	3: "HEURISTICS",
}
var RewriterConfig_MemOptType_value = map[string]int32{
	"DEFAULT_MEM_OPT":          0,
	"NO_MEM_OPT":               1,
	"MANUAL":                   2,
	"SWAPPING_HEURISTICS":      4,
	"RECOMPUTATION_HEURISTICS": 5,
	"SCHEDULING_HEURISTICS":    6,
	"HEURISTICS":               3,
}

func (x RewriterConfig_MemOptType) String() string {
	return proto.EnumName(RewriterConfig_MemOptType_name, int32(x))
}
func (RewriterConfig_MemOptType) EnumDescriptor() ([]byte, []int) {
	return fileDescriptorRewriterConfig, []int{2, 2}
}

type AutoParallelOptions struct {
	Enable      bool  `protobuf:"varint,1,opt,name=enable,proto3" json:"enable,omitempty"`
	NumReplicas int32 `protobuf:"varint,2,opt,name=num_replicas,json=numReplicas,proto3" json:"num_replicas,omitempty"`
}

func (m *AutoParallelOptions) Reset()         { *m = AutoParallelOptions{} }
func (m *AutoParallelOptions) String() string { return proto.CompactTextString(m) }
func (*AutoParallelOptions) ProtoMessage()    {}
func (*AutoParallelOptions) Descriptor() ([]byte, []int) {
	return fileDescriptorRewriterConfig, []int{0}
}

func (m *AutoParallelOptions) GetEnable() bool {
	if m != nil {
		return m.Enable
	}
	return false
}

func (m *AutoParallelOptions) GetNumReplicas() int32 {
	if m != nil {
		return m.NumReplicas
	}
	return 0
}

type ScopedAllocatorOptions struct {
	// If present, only perform optimization for these ops.
	EnableOp []string `protobuf:"bytes,1,rep,name=enable_op,json=enableOp" json:"enable_op,omitempty"`
}

func (m *ScopedAllocatorOptions) Reset()         { *m = ScopedAllocatorOptions{} }
func (m *ScopedAllocatorOptions) String() string { return proto.CompactTextString(m) }
func (*ScopedAllocatorOptions) ProtoMessage()    {}
func (*ScopedAllocatorOptions) Descriptor() ([]byte, []int) {
	return fileDescriptorRewriterConfig, []int{1}
}

func (m *ScopedAllocatorOptions) GetEnableOp() []string {
	if m != nil {
		return m.EnableOp
	}
	return nil
}

type RewriterConfig struct {
	// Optimize tensor layouts (default is ON)
	// e.g. This will try to use NCHW layout on GPU which is faster.
	LayoutOptimizer RewriterConfig_Toggle `protobuf:"varint,1,opt,name=layout_optimizer,json=layoutOptimizer,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"layout_optimizer,omitempty"`
	// Fold constants (default is ON)
	// Statically infer the value of tensors when possible, and materialize the
	// result using constants.
	ConstantFolding RewriterConfig_Toggle `protobuf:"varint,3,opt,name=constant_folding,json=constantFolding,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"constant_folding,omitempty"`
	// Shape optimizations (default is ON)
	// Simplify computations made on shapes.
	ShapeOptimization RewriterConfig_Toggle `protobuf:"varint,13,opt,name=shape_optimization,json=shapeOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"shape_optimization,omitempty"`
	// Remapping (default is ON)
	// Remap subgraphs onto more efficient implementations.
	Remapping RewriterConfig_Toggle `protobuf:"varint,14,opt,name=remapping,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"remapping,omitempty"`
	// Arithmetic optimizations (default is ON)
	// e.g. Simplify arithmetic ops; merge ops with same value (like constants).
	ArithmeticOptimization RewriterConfig_Toggle `protobuf:"varint,7,opt,name=arithmetic_optimization,json=arithmeticOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"arithmetic_optimization,omitempty"`
	// Control dependency optimizations (default is ON).
	// Remove redundant control dependencies, which may enable other optimization.
	DependencyOptimization RewriterConfig_Toggle `protobuf:"varint,8,opt,name=dependency_optimization,json=dependencyOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"dependency_optimization,omitempty"`
	// Loop optimizations (default is ON).
	LoopOptimization RewriterConfig_Toggle `protobuf:"varint,9,opt,name=loop_optimization,json=loopOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"loop_optimization,omitempty"`
	// Function optimizations (default is ON).
	FunctionOptimization RewriterConfig_Toggle `protobuf:"varint,10,opt,name=function_optimization,json=functionOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"function_optimization,omitempty"`
	// Strips debug-related nodes from the graph (off by default).
	DebugStripper RewriterConfig_Toggle `protobuf:"varint,11,opt,name=debug_stripper,json=debugStripper,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"debug_stripper,omitempty"`
	// If true, don't remove unnecessary ops from the graph
	DisableModelPruning bool `protobuf:"varint,2,opt,name=disable_model_pruning,json=disableModelPruning,proto3" json:"disable_model_pruning,omitempty"`
	// Try to allocate some independent Op outputs contiguously in order to
	// merge or eliminate downstream Ops (off by default).
	ScopedAllocatorOptimization RewriterConfig_Toggle `protobuf:"varint,15,opt,name=scoped_allocator_optimization,json=scopedAllocatorOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"scoped_allocator_optimization,omitempty"`
	// Controls how many times we run the optimizers in meta optimizer (default
	// is once).
	MetaOptimizerIterations RewriterConfig_NumIterationsType `protobuf:"varint,12,opt,name=meta_optimizer_iterations,json=metaOptimizerIterations,proto3,enum=tensorflow.RewriterConfig_NumIterationsType" json:"meta_optimizer_iterations,omitempty"`
	// The minimum number of nodes in a graph to optimizer. For smaller graphs,
	// optimization is skipped.
	// 0 means the system picks an appropriate number.
	// < 0 means do not skip optimization.
	MinGraphNodes int32 `protobuf:"varint,17,opt,name=min_graph_nodes,json=minGraphNodes,proto3" json:"min_graph_nodes,omitempty"`
	// Configures memory optimization passes through the meta-optimizer. Has no
	// effect on manually requested memory optimization passes in the optimizers
	// field.
	MemoryOptimization RewriterConfig_MemOptType `protobuf:"varint,4,opt,name=memory_optimization,json=memoryOptimization,proto3,enum=tensorflow.RewriterConfig_MemOptType" json:"memory_optimization,omitempty"`
	// A node name scope for node names which are valid outputs of recompuations.
	// Inputs to nodes that match this scope may be recomputed (subject either to
	// manual annotation of those input nodes or to manual annotation and
	// heuristics depending on memory_optimization), but the nodes themselves will
	// not be recomputed. This matches any sub-scopes as well, meaning the scope
	// can appear not just as a top-level scope. For example, if the value is
	// "gradients/", the default, it will match node name "gradients/foo",
	// "foo/gradients/bar", but not "foo_gradients/"
	MemoryOptimizerTargetNodeNameScope string `protobuf:"bytes,6,opt,name=memory_optimizer_target_node_name_scope,json=memoryOptimizerTargetNodeNameScope,proto3" json:"memory_optimizer_target_node_name_scope,omitempty"`
	// Configures AutoParallel optimization passes either through the
	// meta-optimizer or when manually specified through the optimizers field.
	AutoParallel        *AutoParallelOptions    `protobuf:"bytes,5,opt,name=auto_parallel,json=autoParallel" json:"auto_parallel,omitempty"`
	ScopedAllocatorOpts *ScopedAllocatorOptions `protobuf:"bytes,16,opt,name=scoped_allocator_opts,json=scopedAllocatorOpts" json:"scoped_allocator_opts,omitempty"`
	// If non-empty, will use this as an alternative way to specify a list of
	// optimizations to turn on and the order of the optimizations (replacing the
	// meta-optimizer).
	//
	// Of the RewriterConfig options, only the AutoParallel configuration options
	// (the auto_parallel field) apply to manually requested optimization passes
	// ("autoparallel"). Memory optimization passes ("memory") invoked here are
	// not configurable (in contrast to memory optimization passes through the
	// meta-optimizer) and act only on manual op annotations.
	//
	// Custom registered optimizers will be run after the base optimizers, in
	// the order that they are specified.
	Optimizers []string `protobuf:"bytes,100,rep,name=optimizers" json:"optimizers,omitempty"`
	// list of CustomGraphOptimizers to apply.
	CustomOptimizers []*RewriterConfig_CustomGraphOptimizer `protobuf:"bytes,200,rep,name=custom_optimizers,json=customOptimizers" json:"custom_optimizers,omitempty"`
}

func (m *RewriterConfig) Reset()                    { *m = RewriterConfig{} }
func (m *RewriterConfig) String() string            { return proto.CompactTextString(m) }
func (*RewriterConfig) ProtoMessage()               {}
func (*RewriterConfig) Descriptor() ([]byte, []int) { return fileDescriptorRewriterConfig, []int{2} }

func (m *RewriterConfig) GetLayoutOptimizer() RewriterConfig_Toggle {
	if m != nil {
		return m.LayoutOptimizer
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetConstantFolding() RewriterConfig_Toggle {
	if m != nil {
		return m.ConstantFolding
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetShapeOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.ShapeOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetRemapping() RewriterConfig_Toggle {
	if m != nil {
		return m.Remapping
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetArithmeticOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.ArithmeticOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetDependencyOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.DependencyOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetLoopOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.LoopOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetFunctionOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.FunctionOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetDebugStripper() RewriterConfig_Toggle {
	if m != nil {
		return m.DebugStripper
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetDisableModelPruning() bool {
	if m != nil {
		return m.DisableModelPruning
	}
	return false
}

func (m *RewriterConfig) GetScopedAllocatorOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.ScopedAllocatorOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetMetaOptimizerIterations() RewriterConfig_NumIterationsType {
	if m != nil {
		return m.MetaOptimizerIterations
	}
	return RewriterConfig_DEFAULT_NUM_ITERS
}

func (m *RewriterConfig) GetMinGraphNodes() int32 {
	if m != nil {
		return m.MinGraphNodes
	}
	return 0
}

func (m *RewriterConfig) GetMemoryOptimization() RewriterConfig_MemOptType {
	if m != nil {
		return m.MemoryOptimization
	}
	return RewriterConfig_DEFAULT_MEM_OPT
}

func (m *RewriterConfig) GetMemoryOptimizerTargetNodeNameScope() string {
	if m != nil {
		return m.MemoryOptimizerTargetNodeNameScope
	}
	return ""
}

func (m *RewriterConfig) GetAutoParallel() *AutoParallelOptions {
	if m != nil {
		return m.AutoParallel
	}
	return nil
}

func (m *RewriterConfig) GetScopedAllocatorOpts() *ScopedAllocatorOptions {
	if m != nil {
		return m.ScopedAllocatorOpts
	}
	return nil
}

func (m *RewriterConfig) GetOptimizers() []string {
	if m != nil {
		return m.Optimizers
	}
	return nil
}

func (m *RewriterConfig) GetCustomOptimizers() []*RewriterConfig_CustomGraphOptimizer {
	if m != nil {
		return m.CustomOptimizers
	}
	return nil
}

// Message to describe custom graph optimizer and its parameters
type RewriterConfig_CustomGraphOptimizer struct {
	Name         string                            `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	ParameterMap map[string]*tensorflow5.AttrValue `protobuf:"bytes,2,rep,name=parameter_map,json=parameterMap" json:"parameter_map,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value"`
}

func (m *RewriterConfig_CustomGraphOptimizer) Reset()         { *m = RewriterConfig_CustomGraphOptimizer{} }
func (m *RewriterConfig_CustomGraphOptimizer) String() string { return proto.CompactTextString(m) }
func (*RewriterConfig_CustomGraphOptimizer) ProtoMessage()    {}
func (*RewriterConfig_CustomGraphOptimizer) Descriptor() ([]byte, []int) {
	return fileDescriptorRewriterConfig, []int{2, 0}
}

func (m *RewriterConfig_CustomGraphOptimizer) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *RewriterConfig_CustomGraphOptimizer) GetParameterMap() map[string]*tensorflow5.AttrValue {
	if m != nil {
		return m.ParameterMap
	}
	return nil
}

func init() {
	proto.RegisterType((*AutoParallelOptions)(nil), "tensorflow.AutoParallelOptions")
	proto.RegisterType((*ScopedAllocatorOptions)(nil), "tensorflow.ScopedAllocatorOptions")
	proto.RegisterType((*RewriterConfig)(nil), "tensorflow.RewriterConfig")
	proto.RegisterType((*RewriterConfig_CustomGraphOptimizer)(nil), "tensorflow.RewriterConfig.CustomGraphOptimizer")
	proto.RegisterEnum("tensorflow.RewriterConfig_Toggle", RewriterConfig_Toggle_name, RewriterConfig_Toggle_value)
	proto.RegisterEnum("tensorflow.RewriterConfig_NumIterationsType", RewriterConfig_NumIterationsType_name, RewriterConfig_NumIterationsType_value)
	proto.RegisterEnum("tensorflow.RewriterConfig_MemOptType", RewriterConfig_MemOptType_name, RewriterConfig_MemOptType_value)
}
func (m *AutoParallelOptions) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *AutoParallelOptions) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.Enable {
		dAtA[i] = 0x8
		i++
		if m.Enable {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i++
	}
	if m.NumReplicas != 0 {
		dAtA[i] = 0x10
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.NumReplicas))
	}
	return i, nil
}

func (m *ScopedAllocatorOptions) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *ScopedAllocatorOptions) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.EnableOp) > 0 {
		for _, s := range m.EnableOp {
			dAtA[i] = 0xa
			i++
			l = len(s)
			for l >= 1<<7 {
				dAtA[i] = uint8(uint64(l)&0x7f | 0x80)
				l >>= 7
				i++
			}
			dAtA[i] = uint8(l)
			i++
			i += copy(dAtA[i:], s)
		}
	}
	return i, nil
}

func (m *RewriterConfig) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *RewriterConfig) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.LayoutOptimizer != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.LayoutOptimizer))
	}
	if m.DisableModelPruning {
		dAtA[i] = 0x10
		i++
		if m.DisableModelPruning {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i++
	}
	if m.ConstantFolding != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ConstantFolding))
	}
	if m.MemoryOptimization != 0 {
		dAtA[i] = 0x20
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.MemoryOptimization))
	}
	if m.AutoParallel != nil {
		dAtA[i] = 0x2a
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.AutoParallel.Size()))
		n1, err := m.AutoParallel.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n1
	}
	if len(m.MemoryOptimizerTargetNodeNameScope) > 0 {
		dAtA[i] = 0x32
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(len(m.MemoryOptimizerTargetNodeNameScope)))
		i += copy(dAtA[i:], m.MemoryOptimizerTargetNodeNameScope)
	}
	if m.ArithmeticOptimization != 0 {
		dAtA[i] = 0x38
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ArithmeticOptimization))
	}
	if m.DependencyOptimization != 0 {
		dAtA[i] = 0x40
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.DependencyOptimization))
	}
	if m.LoopOptimization != 0 {
		dAtA[i] = 0x48
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.LoopOptimization))
	}
	if m.FunctionOptimization != 0 {
		dAtA[i] = 0x50
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.FunctionOptimization))
	}
	if m.DebugStripper != 0 {
		dAtA[i] = 0x58
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.DebugStripper))
	}
	if m.MetaOptimizerIterations != 0 {
		dAtA[i] = 0x60
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.MetaOptimizerIterations))
	}
	if m.ShapeOptimization != 0 {
		dAtA[i] = 0x68
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ShapeOptimization))
	}
	if m.Remapping != 0 {
		dAtA[i] = 0x70
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.Remapping))
	}
	if m.ScopedAllocatorOptimization != 0 {
		dAtA[i] = 0x78
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ScopedAllocatorOptimization))
	}
	if m.ScopedAllocatorOpts != nil {
		dAtA[i] = 0x82
		i++
		dAtA[i] = 0x1
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ScopedAllocatorOpts.Size()))
		n2, err := m.ScopedAllocatorOpts.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n2
	}
	if m.MinGraphNodes != 0 {
		dAtA[i] = 0x88
		i++
		dAtA[i] = 0x1
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.MinGraphNodes))
	}
	if len(m.Optimizers) > 0 {
		for _, s := range m.Optimizers {
			dAtA[i] = 0xa2
			i++
			dAtA[i] = 0x6
			i++
			l = len(s)
			for l >= 1<<7 {
				dAtA[i] = uint8(uint64(l)&0x7f | 0x80)
				l >>= 7
				i++
			}
			dAtA[i] = uint8(l)
			i++
			i += copy(dAtA[i:], s)
		}
	}
	if len(m.CustomOptimizers) > 0 {
		for _, msg := range m.CustomOptimizers {
			dAtA[i] = 0xc2
			i++
			dAtA[i] = 0xc
			i++
			i = encodeVarintRewriterConfig(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	return i, nil
}

func (m *RewriterConfig_CustomGraphOptimizer) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *RewriterConfig_CustomGraphOptimizer) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.Name) > 0 {
		dAtA[i] = 0xa
		i++
		i = encodeVarintRewriterConfig(dAtA, i, uint64(len(m.Name)))
		i += copy(dAtA[i:], m.Name)
	}
	if len(m.ParameterMap) > 0 {
		keysForParameterMap := make([]string, 0, len(m.ParameterMap))
		for k, _ := range m.ParameterMap {
			keysForParameterMap = append(keysForParameterMap, string(k))
		}
		sortkeys.Strings(keysForParameterMap)
		for _, k := range keysForParameterMap {
			dAtA[i] = 0x12
			i++
			v := m.ParameterMap[string(k)]
			msgSize := 0
			if v != nil {
				msgSize = v.Size()
				msgSize += 1 + sovRewriterConfig(uint64(msgSize))
			}
			mapSize := 1 + len(k) + sovRewriterConfig(uint64(len(k))) + msgSize
			i = encodeVarintRewriterConfig(dAtA, i, uint64(mapSize))
			dAtA[i] = 0xa
			i++
			i = encodeVarintRewriterConfig(dAtA, i, uint64(len(k)))
			i += copy(dAtA[i:], k)
			if v != nil {
				dAtA[i] = 0x12
				i++
				i = encodeVarintRewriterConfig(dAtA, i, uint64(v.Size()))
				n3, err := v.MarshalTo(dAtA[i:])
				if err != nil {
					return 0, err
				}
				i += n3
			}
		}
	}
	return i, nil
}

func encodeVarintRewriterConfig(dAtA []byte, offset int, v uint64) int {
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return offset + 1
}
func (m *AutoParallelOptions) Size() (n int) {
	var l int
	_ = l
	if m.Enable {
		n += 2
	}
	if m.NumReplicas != 0 {
		n += 1 + sovRewriterConfig(uint64(m.NumReplicas))
	}
	return n
}

func (m *ScopedAllocatorOptions) Size() (n int) {
	var l int
	_ = l
	if len(m.EnableOp) > 0 {
		for _, s := range m.EnableOp {
			l = len(s)
			n += 1 + l + sovRewriterConfig(uint64(l))
		}
	}
	return n
}

func (m *RewriterConfig) Size() (n int) {
	var l int
	_ = l
	if m.LayoutOptimizer != 0 {
		n += 1 + sovRewriterConfig(uint64(m.LayoutOptimizer))
	}
	if m.DisableModelPruning {
		n += 2
	}
	if m.ConstantFolding != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ConstantFolding))
	}
	if m.MemoryOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.MemoryOptimization))
	}
	if m.AutoParallel != nil {
		l = m.AutoParallel.Size()
		n += 1 + l + sovRewriterConfig(uint64(l))
	}
	l = len(m.MemoryOptimizerTargetNodeNameScope)
	if l > 0 {
		n += 1 + l + sovRewriterConfig(uint64(l))
	}
	if m.ArithmeticOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ArithmeticOptimization))
	}
	if m.DependencyOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.DependencyOptimization))
	}
	if m.LoopOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.LoopOptimization))
	}
	if m.FunctionOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.FunctionOptimization))
	}
	if m.DebugStripper != 0 {
		n += 1 + sovRewriterConfig(uint64(m.DebugStripper))
	}
	if m.MetaOptimizerIterations != 0 {
		n += 1 + sovRewriterConfig(uint64(m.MetaOptimizerIterations))
	}
	if m.ShapeOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ShapeOptimization))
	}
	if m.Remapping != 0 {
		n += 1 + sovRewriterConfig(uint64(m.Remapping))
	}
	if m.ScopedAllocatorOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ScopedAllocatorOptimization))
	}
	if m.ScopedAllocatorOpts != nil {
		l = m.ScopedAllocatorOpts.Size()
		n += 2 + l + sovRewriterConfig(uint64(l))
	}
	if m.MinGraphNodes != 0 {
		n += 2 + sovRewriterConfig(uint64(m.MinGraphNodes))
	}
	if len(m.Optimizers) > 0 {
		for _, s := range m.Optimizers {
			l = len(s)
			n += 2 + l + sovRewriterConfig(uint64(l))
		}
	}
	if len(m.CustomOptimizers) > 0 {
		for _, e := range m.CustomOptimizers {
			l = e.Size()
			n += 2 + l + sovRewriterConfig(uint64(l))
		}
	}
	return n
}

func (m *RewriterConfig_CustomGraphOptimizer) Size() (n int) {
	var l int
	_ = l
	l = len(m.Name)
	if l > 0 {
		n += 1 + l + sovRewriterConfig(uint64(l))
	}
	if len(m.ParameterMap) > 0 {
		for k, v := range m.ParameterMap {
			_ = k
			_ = v
			l = 0
			if v != nil {
				l = v.Size()
				l += 1 + sovRewriterConfig(uint64(l))
			}
			mapEntrySize := 1 + len(k) + sovRewriterConfig(uint64(len(k))) + l
			n += mapEntrySize + 1 + sovRewriterConfig(uint64(mapEntrySize))
		}
	}
	return n
}

func sovRewriterConfig(x uint64) (n int) {
	for {
		n++
		x >>= 7
		if x == 0 {
			break
		}
	}
	return n
}
func sozRewriterConfig(x uint64) (n int) {
	return sovRewriterConfig(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}
func (m *AutoParallelOptions) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: AutoParallelOptions: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: AutoParallelOptions: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Enable", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Enable = bool(v != 0)
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field NumReplicas", wireType)
			}
			m.NumReplicas = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.NumReplicas |= (int32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *ScopedAllocatorOptions) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: ScopedAllocatorOptions: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: ScopedAllocatorOptions: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field EnableOp", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.EnableOp = append(m.EnableOp, string(dAtA[iNdEx:postIndex]))
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *RewriterConfig) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: RewriterConfig: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: RewriterConfig: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LayoutOptimizer", wireType)
			}
			m.LayoutOptimizer = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LayoutOptimizer |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DisableModelPruning", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.DisableModelPruning = bool(v != 0)
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ConstantFolding", wireType)
			}
			m.ConstantFolding = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ConstantFolding |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MemoryOptimization", wireType)
			}
			m.MemoryOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MemoryOptimization |= (RewriterConfig_MemOptType(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AutoParallel", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.AutoParallel == nil {
				m.AutoParallel = &AutoParallelOptions{}
			}
			if err := m.AutoParallel.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field MemoryOptimizerTargetNodeNameScope", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.MemoryOptimizerTargetNodeNameScope = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 7:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ArithmeticOptimization", wireType)
			}
			m.ArithmeticOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ArithmeticOptimization |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 8:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DependencyOptimization", wireType)
			}
			m.DependencyOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DependencyOptimization |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LoopOptimization", wireType)
			}
			m.LoopOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LoopOptimization |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 10:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FunctionOptimization", wireType)
			}
			m.FunctionOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FunctionOptimization |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 11:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DebugStripper", wireType)
			}
			m.DebugStripper = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DebugStripper |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 12:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MetaOptimizerIterations", wireType)
			}
			m.MetaOptimizerIterations = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MetaOptimizerIterations |= (RewriterConfig_NumIterationsType(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 13:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ShapeOptimization", wireType)
			}
			m.ShapeOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ShapeOptimization |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 14:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Remapping", wireType)
			}
			m.Remapping = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Remapping |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 15:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ScopedAllocatorOptimization", wireType)
			}
			m.ScopedAllocatorOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ScopedAllocatorOptimization |= (RewriterConfig_Toggle(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 16:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field ScopedAllocatorOpts", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.ScopedAllocatorOpts == nil {
				m.ScopedAllocatorOpts = &ScopedAllocatorOptions{}
			}
			if err := m.ScopedAllocatorOpts.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 17:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MinGraphNodes", wireType)
			}
			m.MinGraphNodes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MinGraphNodes |= (int32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 100:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Optimizers", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Optimizers = append(m.Optimizers, string(dAtA[iNdEx:postIndex]))
			iNdEx = postIndex
		case 200:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field CustomOptimizers", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.CustomOptimizers = append(m.CustomOptimizers, &RewriterConfig_CustomGraphOptimizer{})
			if err := m.CustomOptimizers[len(m.CustomOptimizers)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *RewriterConfig_CustomGraphOptimizer) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: CustomGraphOptimizer: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: CustomGraphOptimizer: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Name", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Name = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field ParameterMap", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.ParameterMap == nil {
				m.ParameterMap = make(map[string]*tensorflow5.AttrValue)
			}
			var mapkey string
			var mapvalue *tensorflow5.AttrValue
			for iNdEx < postIndex {
				entryPreIndex := iNdEx
				var wire uint64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowRewriterConfig
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					wire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				fieldNum := int32(wire >> 3)
				if fieldNum == 1 {
					var stringLenmapkey uint64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRewriterConfig
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						stringLenmapkey |= (uint64(b) & 0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					intStringLenmapkey := int(stringLenmapkey)
					if intStringLenmapkey < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					postStringIndexmapkey := iNdEx + intStringLenmapkey
					if postStringIndexmapkey > l {
						return io.ErrUnexpectedEOF
					}
					mapkey = string(dAtA[iNdEx:postStringIndexmapkey])
					iNdEx = postStringIndexmapkey
				} else if fieldNum == 2 {
					var mapmsglen int
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRewriterConfig
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapmsglen |= (int(b) & 0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					if mapmsglen < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					postmsgIndex := iNdEx + mapmsglen
					if mapmsglen < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					if postmsgIndex > l {
						return io.ErrUnexpectedEOF
					}
					mapvalue = &tensorflow5.AttrValue{}
					if err := mapvalue.Unmarshal(dAtA[iNdEx:postmsgIndex]); err != nil {
						return err
					}
					iNdEx = postmsgIndex
				} else {
					iNdEx = entryPreIndex
					skippy, err := skipRewriterConfig(dAtA[iNdEx:])
					if err != nil {
						return err
					}
					if skippy < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					if (iNdEx + skippy) > postIndex {
						return io.ErrUnexpectedEOF
					}
					iNdEx += skippy
				}
			}
			m.ParameterMap[mapkey] = mapvalue
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func skipRewriterConfig(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
			return iNdEx, nil
		case 1:
			iNdEx += 8
			return iNdEx, nil
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			iNdEx += length
			if length < 0 {
				return 0, ErrInvalidLengthRewriterConfig
			}
			return iNdEx, nil
		case 3:
			for {
				var innerWire uint64
				var start int = iNdEx
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return 0, ErrIntOverflowRewriterConfig
					}
					if iNdEx >= l {
						return 0, io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					innerWire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				innerWireType := int(innerWire & 0x7)
				if innerWireType == 4 {
					break
				}
				next, err := skipRewriterConfig(dAtA[start:])
				if err != nil {
					return 0, err
				}
				iNdEx = start + next
			}
			return iNdEx, nil
		case 4:
			return iNdEx, nil
		case 5:
			iNdEx += 4
			return iNdEx, nil
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
	}
	panic("unreachable")
}

var (
	ErrInvalidLengthRewriterConfig = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowRewriterConfig   = fmt.Errorf("proto: integer overflow")
)

func init() {
	proto.RegisterFile("tensorflow/core/protobuf/rewriter_config.proto", fileDescriptorRewriterConfig)
}

var fileDescriptorRewriterConfig = []byte{
	// 1026 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x94, 0x96, 0xdd, 0x6a, 0xe3, 0x46,
	0x14, 0xc7, 0x57, 0xf6, 0xc6, 0x89, 0x8f, 0x13, 0x47, 0x1e, 0xe7, 0x43, 0x9b, 0x6d, 0xd3, 0xac,
	0xa1, 0x6d, 0x68, 0x8b, 0x0d, 0x29, 0x2d, 0xa5, 0x2c, 0x14, 0x6f, 0xe2, 0x24, 0x86, 0xd8, 0x12,
	0xb2, 0x9d, 0x85, 0x40, 0x11, 0x13, 0x69, 0xec, 0x88, 0x95, 0x34, 0xc3, 0x68, 0xd4, 0x90, 0xde,
	0xf6, 0x41, 0xfa, 0x3a, 0x7b, 0xd9, 0xcb, 0x5e, 0x96, 0x40, 0x6f, 0xfa, 0x04, 0xbd, 0x2c, 0x33,
	0xf2, 0x87, 0x94, 0x84, 0xe0, 0xde, 0xcd, 0xc7, 0xf9, 0xff, 0xe6, 0xcc, 0x39, 0x33, 0x67, 0x06,
	0x9a, 0x82, 0x44, 0x31, 0xe5, 0xe3, 0x80, 0xde, 0xb6, 0x5c, 0xca, 0x49, 0x8b, 0x71, 0x2a, 0xe8,
	0x75, 0x32, 0x6e, 0x71, 0x72, 0xcb, 0x7d, 0x41, 0xb8, 0xe3, 0xd2, 0x68, 0xec, 0x4f, 0x9a, 0x6a,
	0x02, 0xc1, 0xc2, 0x7e, 0xef, 0xab, 0x87, 0xda, 0x31, 0xc7, 0x21, 0xb9, 0xa5, 0xfc, 0x43, 0x0b,
	0x0b, 0xc1, 0x9d, 0x5f, 0x70, 0x90, 0x90, 0x54, 0xd7, 0xb0, 0xa0, 0xde, 0x4e, 0x04, 0xb5, 0x30,
	0xc7, 0x41, 0x40, 0x02, 0x93, 0x09, 0x9f, 0x46, 0x31, 0xda, 0x81, 0x12, 0x89, 0xf0, 0x75, 0x40,
	0x0c, 0xed, 0x40, 0x3b, 0x5c, 0xb3, 0xa7, 0x3d, 0xf4, 0x06, 0xd6, 0xa3, 0x24, 0x74, 0x38, 0x61,
	0x81, 0xef, 0xe2, 0xd8, 0x28, 0x1c, 0x68, 0x87, 0x2b, 0x76, 0x25, 0x4a, 0x42, 0x7b, 0x3a, 0xd4,
	0xf8, 0x0e, 0x76, 0x06, 0x2e, 0x65, 0xc4, 0x6b, 0x07, 0x01, 0x75, 0xb1, 0xa0, 0x7c, 0x06, 0x7d,
	0x0d, 0xe5, 0x14, 0xe3, 0x50, 0x66, 0x68, 0x07, 0xc5, 0xc3, 0xb2, 0xbd, 0x96, 0x0e, 0x98, 0xac,
	0xf1, 0xf7, 0x26, 0x54, 0xed, 0xe9, 0xd6, 0x8e, 0xd5, 0xce, 0xd0, 0x05, 0xe8, 0x01, 0xbe, 0xa3,
	0x89, 0x70, 0x28, 0x13, 0x7e, 0xe8, 0xff, 0x4a, 0xb8, 0x72, 0xa7, 0x7a, 0xf4, 0x26, 0x13, 0x9e,
	0x66, 0x5e, 0xd5, 0x1c, 0xd2, 0xc9, 0x24, 0x20, 0xf6, 0x66, 0x2a, 0x35, 0x67, 0x4a, 0x74, 0x04,
	0xdb, 0x9e, 0x1f, 0xab, 0xe5, 0x43, 0xea, 0x91, 0xc0, 0x61, 0x3c, 0x89, 0xfc, 0x68, 0xa2, 0xf6,
	0xb0, 0x66, 0xd7, 0xa7, 0x93, 0x3d, 0x39, 0x67, 0xa5, 0x53, 0xd2, 0x03, 0x97, 0x46, 0xb1, 0xc0,
	0x91, 0x70, 0xc6, 0x34, 0xf0, 0xa4, 0x79, 0x71, 0x69, 0x0f, 0x66, 0xd2, 0xd3, 0x54, 0x89, 0x2e,
	0xa1, 0x1e, 0x92, 0x90, 0xf2, 0xbb, 0xd9, 0x7e, 0xb0, 0x8c, 0x8b, 0xf1, 0x52, 0x01, 0x3f, 0x7f,
	0x06, 0xd8, 0x23, 0xa1, 0xc9, 0xc4, 0xf0, 0x8e, 0x11, 0x1b, 0xa5, 0x04, 0x33, 0x03, 0x40, 0x27,
	0xb0, 0x81, 0x13, 0x41, 0x1d, 0x36, 0x4d, 0xa2, 0xb1, 0x72, 0xa0, 0x1d, 0x56, 0x8e, 0x3e, 0xcb,
	0x12, 0x9f, 0x48, 0xb2, 0xbd, 0x8e, 0x33, 0x83, 0x68, 0x00, 0x5f, 0xe6, 0xbd, 0x23, 0xdc, 0x11,
	0x98, 0x4f, 0x88, 0x70, 0x22, 0xea, 0x11, 0x27, 0xc2, 0x21, 0x71, 0x62, 0x99, 0x59, 0xa3, 0x74,
	0xa0, 0x1d, 0x96, 0xed, 0x46, 0xce, 0x15, 0xc2, 0x87, 0xca, 0xb8, 0x4f, 0x3d, 0xd2, 0xc7, 0x21,
	0x51, 0x67, 0x00, 0x5d, 0xc1, 0x2e, 0xe6, 0xbe, 0xb8, 0x09, 0x89, 0xf0, 0xdd, 0xfc, 0xb6, 0x57,
	0x97, 0x8d, 0xe3, 0xce, 0x82, 0x90, 0xdb, 0xf6, 0x15, 0xec, 0x7a, 0x84, 0x91, 0xc8, 0x23, 0x91,
	0xfb, 0x20, 0xa4, 0x6b, 0x4b, 0xb3, 0x17, 0x84, 0x1c, 0xbb, 0x0f, 0xb5, 0x80, 0x52, 0x96, 0xa7,
	0x96, 0x97, 0xa5, 0xea, 0x52, 0x9b, 0xe3, 0x5d, 0xc2, 0xf6, 0x38, 0x89, 0x5c, 0xd9, 0xce, 0x33,
	0x61, 0x59, 0xe6, 0xd6, 0x4c, 0x9f, 0xe3, 0x9e, 0x43, 0xd5, 0x23, 0xd7, 0xc9, 0xc4, 0x89, 0x05,
	0xf7, 0x19, 0x23, 0xdc, 0xa8, 0x2c, 0x0b, 0xdc, 0x50, 0xc2, 0xc1, 0x54, 0x87, 0x6e, 0xe0, 0x55,
	0x48, 0x04, 0xce, 0x24, 0x5f, 0x4a, 0xd4, 0x2a, 0xb1, 0xb1, 0xae, 0xa0, 0xdf, 0x3c, 0x03, 0xed,
	0x27, 0x61, 0x77, 0x6e, 0xaf, 0x4e, 0xea, 0xae, 0xc4, 0xcd, 0x0f, 0xc7, 0x62, 0x12, 0x59, 0x80,
	0xe2, 0x1b, 0xcc, 0x48, 0x3e, 0x10, 0x1b, 0xcb, 0xfa, 0x5d, 0x53, 0xe2, 0x5c, 0x14, 0x7e, 0x82,
	0x32, 0x27, 0x21, 0x66, 0x4c, 0xde, 0xcf, 0xea, 0xb2, 0xa0, 0x85, 0x06, 0x11, 0xf8, 0x54, 0x9d,
	0x6c, 0xcf, 0xc1, 0xb3, 0xa2, 0x95, 0xf7, 0x6e, 0x73, 0x59, 0xe8, 0xeb, 0xf8, 0x71, 0xed, 0xcb,
	0x9e, 0x82, 0xa7, 0x96, 0x89, 0x0d, 0x5d, 0x5d, 0xd8, 0x46, 0x16, 0xff, 0x74, 0x0d, 0xb5, 0xeb,
	0x8f, 0xf9, 0x31, 0xfa, 0x02, 0x36, 0x43, 0x3f, 0x72, 0x26, 0x1c, 0xb3, 0x1b, 0x75, 0x59, 0x63,
	0xa3, 0xa6, 0x0a, 0xf3, 0x46, 0xe8, 0x47, 0x67, 0x72, 0x54, 0xde, 0xca, 0x18, 0xed, 0x03, 0xcc,
	0xd3, 0x1b, 0x1b, 0x9e, 0xaa, 0xc0, 0x99, 0x11, 0xf4, 0x33, 0xd4, 0xdc, 0x24, 0x16, 0x34, 0x74,
	0x32, 0x66, 0x1f, 0x65, 0xa5, 0xae, 0x1c, 0xb5, 0x9e, 0xd9, 0xfb, 0xb1, 0x12, 0xa9, 0x85, 0xe6,
	0x09, 0xb7, 0xf5, 0x14, 0x35, 0x1f, 0x88, 0xf7, 0xfe, 0xd1, 0x60, 0xeb, 0x29, 0x53, 0x84, 0xe0,
	0xa5, 0xac, 0x2e, 0xaa, 0xb8, 0x97, 0x6d, 0xd5, 0x46, 0x63, 0xd8, 0x90, 0xf5, 0x2c, 0x24, 0xf2,
	0xa9, 0x0b, 0x31, 0x33, 0x0a, 0xca, 0x8d, 0xf6, 0xff, 0x74, 0xa3, 0x69, 0xcd, 0x20, 0x3d, 0xcc,
	0x3a, 0x91, 0xe0, 0x77, 0xf6, 0x3a, 0xcb, 0x0c, 0xed, 0x5d, 0x42, 0xed, 0x91, 0x09, 0xd2, 0xa1,
	0xf8, 0x81, 0xdc, 0x4d, 0xfd, 0x91, 0x4d, 0xf4, 0x35, 0xac, 0xa8, 0x67, 0x53, 0xbd, 0x16, 0x95,
	0xa3, 0xed, 0x5c, 0x6d, 0x15, 0x82, 0x5f, 0xca, 0x49, 0x3b, 0xb5, 0xf9, 0xb1, 0xf0, 0x83, 0xd6,
	0xf8, 0x1e, 0x4a, 0xe9, 0x91, 0x40, 0x15, 0x58, 0x3d, 0xe9, 0x9c, 0xb6, 0x47, 0x17, 0x43, 0xfd,
	0x05, 0x2a, 0x41, 0xc1, 0xec, 0xeb, 0x1a, 0x5a, 0x85, 0xa2, 0x79, 0x7a, 0xaa, 0x17, 0x50, 0x15,
	0xa0, 0x7d, 0x76, 0x66, 0x77, 0x06, 0x83, 0xee, 0x65, 0x47, 0x2f, 0x36, 0xde, 0x42, 0xed, 0xd1,
	0x5d, 0x42, 0xdb, 0x50, 0x9b, 0x22, 0x9c, 0xfe, 0xa8, 0xe7, 0x74, 0x87, 0x1d, 0x7b, 0xa0, 0xbf,
	0x50, 0x90, 0x7e, 0x27, 0xa5, 0x0d, 0xdf, 0x9b, 0x7a, 0xa1, 0xf1, 0xbb, 0x06, 0xb0, 0x78, 0x2d,
	0x50, 0x1d, 0x36, 0x67, 0xba, 0x5e, 0xa7, 0xe7, 0x98, 0x96, 0x74, 0xa1, 0x0a, 0xd0, 0x37, 0xe7,
	0x7d, 0x0d, 0x01, 0x94, 0x7a, 0xed, 0xfe, 0xa8, 0x7d, 0xa1, 0x17, 0xd0, 0x2e, 0xd4, 0x07, 0xef,
	0xdb, 0x96, 0xd5, 0xed, 0x9f, 0x39, 0xe7, 0x9d, 0x91, 0xdd, 0x1d, 0x0c, 0xbb, 0xc7, 0x03, 0xfd,
	0x25, 0xfa, 0x04, 0x0c, 0xbb, 0x73, 0x6c, 0xf6, 0xac, 0xd1, 0xb0, 0x3d, 0xec, 0x9a, 0xfd, 0xec,
	0xec, 0x0a, 0x7a, 0x05, 0xdb, 0x83, 0xe3, 0xf3, 0xce, 0xc9, 0xe8, 0xe2, 0x81, 0xb0, 0x24, 0x57,
	0xcb, 0xf4, 0x8b, 0xef, 0x7e, 0xd3, 0x3e, 0xde, 0xef, 0x6b, 0x7f, 0xdc, 0xef, 0x6b, 0x7f, 0xde,
	0xef, 0x6b, 0x7f, 0xdd, 0xef, 0x6b, 0x60, 0x50, 0x3e, 0xc9, 0xc6, 0x73, 0xfe, 0x5d, 0x79, 0xb7,
	0x95, 0xcf, 0xb0, 0x25, 0xbf, 0x2c, 0xb1, 0xa5, 0x5d, 0xbd, 0x9d, 0xf8, 0xe2, 0x26, 0xb9, 0x6e,
	0xba, 0x34, 0x6c, 0x65, 0x3e, 0x3b, 0x4f, 0x37, 0x27, 0x34, 0xff, 0x83, 0xfa, 0x57, 0xd3, 0xae,
	0x4b, 0xaa, 0xf3, 0xed, 0x7f, 0x01, 0x00, 0x00, 0xff, 0xff, 0x9a, 0xb8, 0x71, 0xae, 0x67, 0x09,
	0x00, 0x00,
}
